{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e773096",
   "metadata": {},
   "source": [
    "## üì¶ Imports et Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "743a78ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ NLTK configur√©\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import nltk\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('punkt_tab', quiet=True)\n",
    "    print(\"‚úÖ NLTK configur√©\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è NLTK setup warning: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1649e32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4150255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.nlp import TextData\n",
    "from deepchecks.nlp.suites import (\n",
    "    data_integrity,\n",
    "    train_test_validation,\n",
    "    model_evaluation\n",
    ")\n",
    "from deepchecks.nlp.checks import (\n",
    "    TextPropertyOutliers,\n",
    "    UnknownTokens,\n",
    "    ConflictingLabels,\n",
    "    LabelDrift,\n",
    "    PropertyDrift,\n",
    "    PredictionDrift,\n",
    "    TextDuplicates\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb318227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e28364e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üîç DEEPCHECKS NLP - VALIDATION MLOps Election\n",
      "================================================================================\n",
      "üìÖ Date: 2025-12-16 17:53:49\n",
      "üìÅ Base: c:\\Users\\user\\Downloads\\Notre_Mlops\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Configuration des chemins\n",
    "BASE_DIR = Path.cwd().parent if Path.cwd().name == 'testing' else Path.cwd()\n",
    "PROCESSOR_DIR = BASE_DIR / 'processors'\n",
    "MODELS_DIR = BASE_DIR / 'models'\n",
    "TESTING_DIR = BASE_DIR / 'testing'\n",
    "TESTING_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üîç DEEPCHECKS NLP - VALIDATION MLOps Election\")\n",
    "print(\"=\"*80)\n",
    "print(f\"üìÖ Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üìÅ Base: {BASE_DIR}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2305b3b1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì• Chargement des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "189411ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_data():\n",
    "    \"\"\"Charge les donn√©es preprocess√©es\"\"\"\n",
    "    print(\"üì¶ Chargement des donn√©es\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    data_path = PROCESSOR_DIR / 'preprocessed_data.pkl'\n",
    "    if not data_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Donn√©es non trouv√©es: {data_path}\\n\"\n",
    "            \"Ex√©cutez: python scripts/preprocess.py\"\n",
    "        )\n",
    "    \n",
    "    with open(data_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    print(f\"‚úÖ Donn√©es charg√©es:\")\n",
    "    print(f\"   Train: {data['X_train'].shape}\")\n",
    "    print(f\"   Val:   {data['X_val'].shape}\")\n",
    "    print(f\"   Test:  {data['X_test'].shape}\")\n",
    "    print()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "904faae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cleaned_texts():\n",
    "    \"\"\"Charge les textes nettoy√©s\"\"\"\n",
    "    texts_path = PROCESSOR_DIR / 'cleaned_texts.pkl'\n",
    "    if not texts_path.exists():\n",
    "        raise FileNotFoundError(f\"Textes non trouv√©s: {texts_path}\")\n",
    "    \n",
    "    with open(texts_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    return data['cleaned'], data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3d35b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best_model():\n",
    "    \"\"\"Charge le meilleur mod√®le ML\"\"\"\n",
    "    print(\"ü§ñ Chargement du meilleur mod√®le\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Essayer de trouver le meilleur mod√®le\n",
    "    model_files = list(MODELS_DIR.glob('*.pkl'))\n",
    "    \n",
    "    if not model_files:\n",
    "        print(\"‚ö†Ô∏è  Aucun mod√®le trouv√© - chargement du LogisticRegression par d√©faut\")\n",
    "        model_path = MODELS_DIR / 'model_lr.pkl'\n",
    "    else:\n",
    "        # Pour l'exemple, on prend Logistic Regression\n",
    "        model_path = MODELS_DIR / 'model_lr.pkl'\n",
    "        if not model_path.exists():\n",
    "            model_path = model_files[0]\n",
    "    \n",
    "    if model_path.exists():\n",
    "        model = joblib.load(model_path)\n",
    "        print(f\"‚úÖ Mod√®le charg√©: {model_path.name}\")\n",
    "        print(f\"   Type: {type(model).__name__}\")\n",
    "        print()\n",
    "        return model\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Aucun mod√®le disponible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9766b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Chargement des donn√©es\n",
      "--------------------------------------------------------------------------------\n",
      "‚úÖ Donn√©es charg√©es:\n",
      "   Train: (2403, 5000)\n",
      "   Val:   (515, 5000)\n",
      "   Test:  (516, 5000)\n",
      "\n",
      "ü§ñ Chargement du meilleur mod√®le\n",
      "--------------------------------------------------------------------------------\n",
      "‚úÖ Mod√®le charg√©: model_gradient_boosting.pkl\n",
      "   Type: GradientBoostingClassifier\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Charger les donn√©es\n",
    "data = load_preprocessed_data()\n",
    "texts, labels = load_cleaned_texts()\n",
    "model = load_best_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f000e651",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Cr√©ation des TextData pour Deepchecks NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09563f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_data(texts_list, labels_list, split_name='train'):\n",
    "    \"\"\"\n",
    "    Cr√©e un TextData Deepchecks NLP √† partir de textes et labels\n",
    "    \"\"\"\n",
    "    print(f\"üìù Cr√©ation TextData NLP ({split_name})\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Cr√©er le TextData Deepchecks\n",
    "    text_data = TextData(\n",
    "        raw_text=texts_list,\n",
    "        label=labels_list,\n",
    "        task_type='text_classification',\n",
    "        name=f'{split_name}_dataset'\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ TextData cr√©√©:\")\n",
    "    print(f\"   Nombre de textes: {len(texts_list)}\")\n",
    "    print(f\"   Distribution labels: {pd.Series(labels_list).value_counts().to_dict()}\")\n",
    "    print()\n",
    "    \n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4073e77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split effectu√©:\n",
      "  Train: 2403 (70.0%)\n",
      "  Val:   515 (15.0%)\n",
      "  Test:  516 (15.0%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Recr√©er le m√™me split que preprocess.py\n",
    "df = pd.DataFrame({'texts': texts, 'labels': labels})\n",
    "train_df, temp_df = train_test_split(\n",
    "    df, test_size=0.30, random_state=42, stratify=df['labels']\n",
    ")\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, test_size=0.5, random_state=42, stratify=temp_df['labels']\n",
    ")\n",
    "\n",
    "print(f\"Split effectu√©:\")\n",
    "print(f\"  Train: {len(train_df)} ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Val:   {len(val_df)} ({len(val_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Test:  {len(test_df)} ({len(test_df)/len(df)*100:.1f}%)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f36a26c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Cr√©ation TextData NLP (train)\n",
      "--------------------------------------------------------------------------------\n",
      "‚úÖ TextData cr√©√©:\n",
      "   Nombre de textes: 2403\n",
      "   Distribution labels: {0: 1233, 1: 1170}\n",
      "\n",
      "üìù Cr√©ation TextData NLP (test)\n",
      "--------------------------------------------------------------------------------\n",
      "‚úÖ TextData cr√©√©:\n",
      "   Nombre de textes: 516\n",
      "   Distribution labels: {0: 265, 1: 251}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cr√©er les TextData NLP\n",
    "train_text_data = create_text_data(\n",
    "    train_df['texts'].tolist(), \n",
    "    train_df['labels'].tolist(), \n",
    "    'train'\n",
    ")\n",
    "test_text_data = create_text_data(\n",
    "    test_df['texts'].tolist(), \n",
    "    test_df['labels'].tolist(), \n",
    "    'test'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38dcf62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Aper√ßu des TextData:\n",
      "Train: <deepchecks.nlp.text_data.TextData object at 0x0000026E6F406770>\n",
      "Test: <deepchecks.nlp.text_data.TextData object at 0x0000026E0EB21FF0>\n"
     ]
    }
   ],
   "source": [
    "# Aper√ßu des TextData\n",
    "print(\"üìä Aper√ßu des TextData:\")\n",
    "print(f\"Train: {train_text_data}\")\n",
    "print(f\"Test: {test_text_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b33360",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä NIVEAU 1 : TEXT DATA INTEGRITY\n",
    "\n",
    "### V√©rifications d'int√©grit√© des donn√©es textuelles NLP natives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e082abbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_text_integrity_checks(train_data, test_data):\n",
    "    \"\"\"\n",
    "    NIVEAU 1: V√©rifications d'int√©grit√© des donn√©es textuelles (NLP natif)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä NIVEAU 1: TEXT DATA INTEGRITY (NLP Natif)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Suite d'int√©grit√© NLP\n",
    "    integrity_suite = data_integrity()\n",
    "    \n",
    "    print(\"\\nüîç Checks NLP ex√©cut√©s:\")\n",
    "    print(\"   1. Text Property Outliers (longueur, mots rares, etc.)\")\n",
    "    print(\"   2. Unknown Tokens (tokens jamais vus)\")\n",
    "    print(\"   3. Text Duplicates (textes dupliqu√©s)\")\n",
    "    print(\"   4. Conflicting Labels (m√™me texte, labels diff√©rents)\")\n",
    "    print(\"   5. Property Label Correlation\")\n",
    "    \n",
    "    # Ex√©cuter la suite\n",
    "    print(\"\\n‚è≥ Ex√©cution des checks d'int√©grit√© NLP...\")\n",
    "    result = integrity_suite.run(train_data, test_data)\n",
    "    \n",
    "    # Sauvegarder le rapport\n",
    "    integrity_report_path = TESTING_DIR / 'deepchecks_nlp_integrity_report.html'\n",
    "    result.save_as_html(str(integrity_report_path), as_widget=False)\n",
    "    \n",
    "    print(f\"‚úÖ Rapport d'int√©grit√© NLP sauvegard√©: {integrity_report_path.name}\")\n",
    "    \n",
    "    # R√©sum√© des r√©sultats\n",
    "    print(\"\\nüìà R√©sum√© Int√©grit√© NLP:\")\n",
    "    passed = 0\n",
    "    total = 0\n",
    "    for check_result in result.results:\n",
    "        # Skip CheckFailure objects\n",
    "        if hasattr(check_result, 'passed_conditions'):\n",
    "            total += 1\n",
    "            if check_result.passed_conditions():\n",
    "                passed += 1\n",
    "    \n",
    "    if total > 0:\n",
    "        print(f\"   Checks r√©ussies: {passed}/{total}\")\n",
    "    else:\n",
    "        print(f\"   Checks ex√©cut√©s: {len(result.results)}\")\n",
    "    \n",
    "    # Statistiques texte\n",
    "    train_texts = train_data.text\n",
    "    test_texts = test_data.text\n",
    "    \n",
    "    print(\"\\nüìù Statistiques Texte:\")\n",
    "    print(f\"   Train - Longueur moyenne: {np.mean([len(t) for t in train_texts]):.1f} caract√®res\")\n",
    "    print(f\"   Test  - Longueur moyenne: {np.mean([len(t) for t in test_texts]):.1f} caract√®res\")\n",
    "    print(f\"   Train - Mots moyens: {np.mean([len(t.split()) for t in train_texts]):.1f}\")\n",
    "    print(f\"   Test  - Mots moyens: {np.mean([len(t.split()) for t in test_texts]):.1f}\")\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6241c285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä NIVEAU 1: TEXT DATA INTEGRITY (NLP Natif)\n",
      "================================================================================\n",
      "\n",
      "üîç Checks NLP ex√©cut√©s:\n",
      "   1. Text Property Outliers (longueur, mots rares, etc.)\n",
      "   2. Unknown Tokens (tokens jamais vus)\n",
      "   3. Text Duplicates (textes dupliqu√©s)\n",
      "   4. Conflicting Labels (m√™me texte, labels diff√©rents)\n",
      "   5. Property Label Correlation\n",
      "\n",
      "‚è≥ Ex√©cution des checks d'int√©grit√© NLP...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Rapport d'int√©grit√© NLP sauvegard√©: deepchecks_nlp_integrity_report.html\n",
      "\n",
      "üìà R√©sum√© Int√©grit√© NLP:\n",
      "   Checks r√©ussies: 9/10\n",
      "\n",
      "üìù Statistiques Texte:\n",
      "   Train - Longueur moyenne: 82.3 caract√®res\n",
      "   Test  - Longueur moyenne: 83.1 caract√®res\n",
      "   Train - Mots moyens: 15.3\n",
      "   Test  - Mots moyens: 15.4\n"
     ]
    }
   ],
   "source": [
    "# Ex√©cuter les checks d'int√©grit√© NLP\n",
    "integrity_result = run_text_integrity_checks(train_text_data, test_text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8d613e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b06c148cd2c43bb85647220104e8f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(HTML(value='\\n<h1 id=\"summary_AAMSDME7R1GOP6HAVVG0CIGZI\">Data Integrity Sui‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Afficher le r√©sum√© interactif\n",
    "integrity_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c358cfe",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìà NIVEAU 2 : TRAIN-TEST DRIFT\n",
    "\n",
    "### D√©tection de drift s√©mantique et de distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "595598ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nlp_drift_checks(train_data, test_data):\n",
    "    \"\"\"\n",
    "    NIVEAU 2: D√©tection de drift NLP (distribution, propri√©t√©s, labels)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä NIVEAU 2: NLP TRAIN-TEST DRIFT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Calculer les propri√©t√©s built-in pour les checks\n",
    "    print(\"\\n‚è≥ Calcul des propri√©t√©s textuelles...\")\n",
    "    train_data.calculate_builtin_properties()\n",
    "    test_data.calculate_builtin_properties()\n",
    "    print(\"‚úÖ Propri√©t√©s calcul√©es\")\n",
    "    \n",
    "    # Suite de validation train-test NLP\n",
    "    drift_suite = train_test_validation()\n",
    "    \n",
    "    print(\"\\nüîç Checks de Drift NLP (suite compl√®te):\")\n",
    "    print(\"   1. Label Drift (distribution des labels)\")\n",
    "    print(\"   2. Property Drift (longueur texte, vocabulaire)\")\n",
    "    print(\"   3. Text Embeddings Drift\")\n",
    "    print(\"   4. Train Test Samples Mix\")\n",
    "    \n",
    "    # Ex√©cuter la suite\n",
    "    print(\"\\n‚è≥ Ex√©cution des checks de drift NLP...\")\n",
    "    result = drift_suite.run(train_data, test_data)\n",
    "    \n",
    "    # Sauvegarder le rapport\n",
    "    drift_report_path = TESTING_DIR / 'deepchecks_nlp_drift_report.html'\n",
    "    result.save_as_html(str(drift_report_path), as_widget=False)\n",
    "    \n",
    "    print(f\"‚úÖ Rapport de drift NLP sauvegard√©: {drift_report_path.name}\")\n",
    "    \n",
    "    # R√©sum√© des r√©sultats\n",
    "    print(\"\\nüìà R√©sum√© Drift NLP:\")\n",
    "    passed = 0\n",
    "    total = 0\n",
    "    for check_result in result.results:\n",
    "        if hasattr(check_result, 'passed_conditions'):\n",
    "            total += 1\n",
    "            if check_result.passed_conditions():\n",
    "                passed += 1\n",
    "    \n",
    "    if total > 0:\n",
    "        print(f\"   Checks r√©ussies: {passed}/{total}\")\n",
    "    else:\n",
    "        print(f\"   Checks ex√©cut√©s: {len(result.results)}\")\n",
    "    \n",
    "    # Statistiques de distribution\n",
    "    print(\"\\nüìä Distribution des Labels:\")\n",
    "    train_labels = train_data.label\n",
    "    test_labels = test_data.label\n",
    "    \n",
    "    print(\"   Train:\")\n",
    "    print(pd.Series(train_labels).value_counts(normalize=True).to_string())\n",
    "    print(\"   Test:\")\n",
    "    print(pd.Series(test_labels).value_counts(normalize=True).to_string())\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46b140f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä NIVEAU 2: NLP TRAIN-TEST DRIFT\n",
      "================================================================================\n",
      "\n",
      "‚è≥ Calcul des propri√©t√©s textuelles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 151/151 [00:01<00:00, 120.78it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:00<00:00, 133.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Propri√©t√©s calcul√©es\n",
      "\n",
      "üîç Checks de Drift NLP (suite compl√®te):\n",
      "   1. Label Drift (distribution des labels)\n",
      "   2. Property Drift (longueur texte, vocabulaire)\n",
      "   3. Text Embeddings Drift\n",
      "   4. Train Test Samples Mix\n",
      "\n",
      "‚è≥ Ex√©cution des checks de drift NLP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deepchecks - WARNING - Could not find model's classes, using the observed classes. In order to make sure the classes used by the model are inferred correctly, please use the model_classes argument\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Rapport de drift NLP sauvegard√©: deepchecks_nlp_drift_report.html\n",
      "\n",
      "üìà R√©sum√© Drift NLP:\n",
      "   Checks r√©ussies: 3/3\n",
      "\n",
      "üìä Distribution des Labels:\n",
      "   Train:\n",
      "0    0.513109\n",
      "1    0.486891\n",
      "   Test:\n",
      "0    0.513566\n",
      "1    0.486434\n"
     ]
    }
   ],
   "source": [
    "# Ex√©cuter les checks de drift NLP\n",
    "drift_result = run_nlp_drift_checks(train_text_data, test_text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aedeb1d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5bc34864359412795043b1646728d88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(HTML(value='\\n<h1 id=\"summary_92PAEAYRM6PEO8FB8NMJY548J\">Train Test Validat‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Afficher le r√©sum√© interactif du drift\n",
    "drift_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e431c808",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèÜ NIVEAU 3 : MODEL PERFORMANCE NLP\n",
    "\n",
    "### √âvaluation de la performance du mod√®le avec m√©triques NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6dac672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nlp_model_performance(model, train_data, test_data, X_train, X_test):\n",
    "    \"\"\"\n",
    "    NIVEAU 3: √âvaluation de la performance du mod√®le NLP\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä NIVEAU 3: MODEL PERFORMANCE NLP\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Faire les pr√©dictions\n",
    "    print(\"\\nüîÆ G√©n√©ration des pr√©dictions...\")\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculer les probabilit√©s si possible\n",
    "    try:\n",
    "        y_train_proba = model.predict_proba(X_train)\n",
    "        y_test_proba = model.predict_proba(X_test)\n",
    "        has_proba = True\n",
    "    except:\n",
    "        y_train_proba = None\n",
    "        y_test_proba = None\n",
    "        has_proba = False\n",
    "\n",
    "    # Normaliser y_true et y_pred en numpy arrays\n",
    "    y_train_true = np.array(train_data.label)\n",
    "    y_test_true = np.array(test_data.label)\n",
    "\n",
    "    # Align prediction types with true labels\n",
    "    def _align_preds_to_labels(preds, labels):\n",
    "        preds_arr = np.array(preds).ravel()\n",
    "        if len(labels) == 0:\n",
    "            return preds_arr\n",
    "        sample = labels[0]\n",
    "        # If labels are strings, cast preds to str\n",
    "        if isinstance(sample, str):\n",
    "            return np.array([str(p) for p in preds_arr])\n",
    "        # If labels are ints, try to cast preds to int\n",
    "        if isinstance(sample, (int, np.integer)):\n",
    "            try:\n",
    "                return np.array([int(p) for p in preds_arr])\n",
    "            except Exception:\n",
    "                return preds_arr\n",
    "        return preds_arr\n",
    "\n",
    "    y_train_pred = _align_preds_to_labels(y_train_pred, y_train_true)\n",
    "    y_test_pred = _align_preds_to_labels(y_test_pred, y_test_true)\n",
    "\n",
    "    # Cr√©er TextData pour les checks de performance\n",
    "    train_data_with_pred = TextData(\n",
    "        raw_text=train_data.text,\n",
    "        label=train_data.label,\n",
    "        task_type='text_classification',\n",
    "        name='train_with_predictions'\n",
    "    )\n",
    "    test_data_with_pred = TextData(\n",
    "        raw_text=test_data.text,\n",
    "        label=test_data.label,\n",
    "        task_type='text_classification',\n",
    "        name='test_with_predictions'\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ Pr√©dictions g√©n√©r√©es\")\n",
    "    print(f\"   Train predictions shape: {y_train_pred.shape}\")\n",
    "    print(f\"   Test predictions shape: {y_test_pred.shape}\")\n",
    "    \n",
    "    # Calculer les propri√©t√©s pour les checks de performance\n",
    "    print(\"\\n‚è≥ Calcul des propri√©t√©s textuelles pour l'√©valuation...\")\n",
    "    train_data_with_pred.calculate_builtin_properties()\n",
    "    test_data_with_pred.calculate_builtin_properties()\n",
    "    print(\"‚úÖ Propri√©t√©s calcul√©es\")\n",
    "    \n",
    "    # Suite d'√©valuation du mod√®le NLP\n",
    "    performance_suite = model_evaluation()\n",
    "    \n",
    "    print(\"\\nüîç Checks de Performance (suite compl√®te):\")\n",
    "    print(\"   1. Prediction Drift\")\n",
    "    print(\"   2. Train Test Performance\")\n",
    "    print(\"   3. Property Segments Performance\")\n",
    "    print(\"   4. Metrics sklearn (int√©gr√©s)\")\n",
    "    \n",
    "    # Ex√©cuter la suite avec pr√©dictions pass√©es en arguments\n",
    "    print(\"\\n‚è≥ Ex√©cution de la suite d'√©valuation...\")\n",
    "    try:\n",
    "        # Pass predictions directly to the run() method!\n",
    "        performance_result = performance_suite.run(\n",
    "            train_dataset=train_data_with_pred, \n",
    "            test_dataset=test_data_with_pred,\n",
    "            train_predictions=list(y_train_pred),\n",
    "            test_predictions=list(y_test_pred),\n",
    "            train_probabilities=y_train_proba if has_proba else None,\n",
    "            test_probabilities=y_test_proba if has_proba else None\n",
    "        )\n",
    "        print(\"‚úÖ Suite d'√©valuation ex√©cut√©e\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Suite d'√©valuation erreur: {str(e)[:150]}\")\n",
    "        print(\"   ‚ÑπÔ∏è  Les m√©triques de performance sont calcul√©es ci-dessous\")\n",
    "        performance_result = None\n",
    "    \n",
    "    # M√©triques custom\n",
    "    print(\"\\nüèÜ M√©triques du Mod√®le:\")\n",
    "    \n",
    "    # Determine pos_label based on label type\n",
    "    sample_label = train_data.label[0]\n",
    "    if isinstance(sample_label, str):\n",
    "        pos_label = '1'\n",
    "    else:\n",
    "        pos_label = 1\n",
    "    \n",
    "    # Train metrics\n",
    "    train_acc = accuracy_score(train_data.label, y_train_pred)\n",
    "    train_f1 = f1_score(train_data.label, y_train_pred, average='binary', pos_label=pos_label)\n",
    "    \n",
    "    # Test metrics\n",
    "    test_acc = accuracy_score(test_data.label, y_test_pred)\n",
    "    test_f1 = f1_score(test_data.label, y_test_pred, average='binary', pos_label=pos_label)\n",
    "    test_precision = precision_score(test_data.label, y_test_pred, average='binary', pos_label=pos_label)\n",
    "    test_recall = recall_score(test_data.label, y_test_pred, average='binary', pos_label=pos_label)\n",
    "    \n",
    "    print(f\"   Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"   Train F1:       {train_f1:.4f}\")\n",
    "    print(f\"   Test Accuracy:  {test_acc:.4f}\")\n",
    "    print(f\"   Test F1:        {test_f1:.4f}\")\n",
    "    print(f\"   Test Precision: {test_precision:.4f}\")\n",
    "    print(f\"   Test Recall:    {test_recall:.4f}\")\n",
    "    \n",
    "    # Overfitting check\n",
    "    overfit_gap = train_acc - test_acc\n",
    "    print(f\"\\n‚ö†Ô∏è  √âcart Train/Test: {overfit_gap:.4f}\")\n",
    "    if overfit_gap > 0.1:\n",
    "        print(\"   ‚ö†Ô∏è  ATTENTION: Possible overfitting d√©tect√©!\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ Pas d'overfitting majeur\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(test_data.label, y_test_pred)\n",
    "    print(f\"\\nüìä Matrice de Confusion (Test):\")\n",
    "    print(cm)\n",
    "    \n",
    "    # Classification report\n",
    "    print(f\"\\nüìã Classification Report (Test):\")\n",
    "    print(classification_report(test_data.label, y_test_pred, \n",
    "                                target_names=['Classe 0', 'Classe 1'], \n",
    "                                digits=4))\n",
    "    \n",
    "    # Sauvegarder le rapport\n",
    "    performance_report_path = TESTING_DIR / 'deepchecks_nlp_performance_report.html'\n",
    "    if performance_result:\n",
    "        performance_result.save_as_html(str(performance_report_path), as_widget=False)\n",
    "        print(f\"\\n‚úÖ Rapport de performance NLP sauvegard√©: {performance_report_path.name}\")\n",
    "    \n",
    "    return performance_result, {\n",
    "        'train_acc': train_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'test_f1': test_f1,\n",
    "        'test_precision': test_precision,\n",
    "        'test_recall': test_recall,\n",
    "        'overfit_gap': overfit_gap\n",
    "    }, train_data_with_pred, test_data_with_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "684b5dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä NIVEAU 3: MODEL PERFORMANCE NLP\n",
      "================================================================================\n",
      "\n",
      "üîÆ G√©n√©ration des pr√©dictions...\n",
      "\n",
      "‚úÖ Pr√©dictions g√©n√©r√©es\n",
      "   Train predictions shape: (2403,)\n",
      "   Test predictions shape: (516,)\n",
      "\n",
      "‚è≥ Calcul des propri√©t√©s textuelles pour l'√©valuation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 151/151 [00:01<00:00, 130.36it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:00<00:00, 145.16it/s]\n",
      "deepchecks - WARNING - Could not find model's classes, using the observed classes. In order to make sure the classes used by the model are inferred correctly, please use the model_classes argument\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Propri√©t√©s calcul√©es\n",
      "\n",
      "üîç Checks de Performance (suite compl√®te):\n",
      "   1. Prediction Drift\n",
      "   2. Train Test Performance\n",
      "   3. Property Segments Performance\n",
      "   4. Metrics sklearn (int√©gr√©s)\n",
      "\n",
      "‚è≥ Ex√©cution de la suite d'√©valuation...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Suite d'√©valuation ex√©cut√©e\n",
      "\n",
      "üèÜ M√©triques du Mod√®le:\n",
      "   Train Accuracy: 0.8556\n",
      "   Train F1:       0.8436\n",
      "   Test Accuracy:  0.7345\n",
      "   Test F1:        0.7079\n",
      "   Test Precision: 0.7615\n",
      "   Test Recall:    0.6614\n",
      "\n",
      "‚ö†Ô∏è  √âcart Train/Test: 0.1211\n",
      "   ‚ö†Ô∏è  ATTENTION: Possible overfitting d√©tect√©!\n",
      "\n",
      "üìä Matrice de Confusion (Test):\n",
      "[[213  52]\n",
      " [ 85 166]]\n",
      "\n",
      "üìã Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Classe 0     0.7148    0.8038    0.7567       265\n",
      "    Classe 1     0.7615    0.6614    0.7079       251\n",
      "\n",
      "    accuracy                         0.7345       516\n",
      "   macro avg     0.7381    0.7326    0.7323       516\n",
      "weighted avg     0.7375    0.7345    0.7329       516\n",
      "\n",
      "\n",
      "‚úÖ Rapport de performance NLP sauvegard√©: deepchecks_nlp_performance_report.html\n"
     ]
    }
   ],
   "source": [
    "# Ex√©cuter les checks de performance NLP\n",
    "performance_result, metrics, train_data_with_pred, test_data_with_pred = run_nlp_model_performance(\n",
    "    model, train_text_data, test_text_data,\n",
    "    data['X_train'], data['X_test']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cac4fa8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç DEBUG: Checking model performance results...\n",
      "Performance result type: <class 'deepchecks.core.suite.SuiteResult'>\n",
      "Train data with predictions type: <class 'deepchecks.nlp.text_data.TextData'>\n",
      "Test data with predictions type: <class 'deepchecks.nlp.text_data.TextData'>\n",
      "\n",
      "‚úÖ Train data attributes with 'pred': []\n",
      "‚úÖ Test data attributes with 'pred': []\n",
      "\n",
      "üìä Performance Result:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afd770be864848508217377035fee5bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(HTML(value='\\n<h1 id=\"summary_1PNT2SBEX0WRQ38CZ14M23YW1\">Model Evaluation S‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Debug: Check predictions and performance results\n",
    "print(\"\\nüîç DEBUG: Checking model performance results...\")\n",
    "print(f\"Performance result type: {type(performance_result)}\")\n",
    "print(f\"Train data with predictions type: {type(train_data_with_pred)}\")\n",
    "print(f\"Test data with predictions type: {type(test_data_with_pred)}\")\n",
    "\n",
    "# Check if predictions are accessible\n",
    "print(f\"\\n‚úÖ Train data attributes with 'pred': {[attr for attr in dir(train_data_with_pred) if 'pred' in attr.lower()]}\")\n",
    "print(f\"‚úÖ Test data attributes with 'pred': {[attr for attr in dir(test_data_with_pred) if 'pred' in attr.lower()]}\")\n",
    "\n",
    "# Display performance result\n",
    "print(\"\\nüìä Performance Result:\")\n",
    "performance_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af3ddc4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä R√©sum√© Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f0f32b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "‚úÖ VALIDATION DEEPCHECKS NLP TERMIN√âE\n",
      "================================================================================\n",
      "\n",
      "üìÇ Rapports NLP g√©n√©r√©s:\n",
      "   1. c:\\Users\\user\\Downloads\\Notre_Mlops\\testing\\deepchecks_nlp_integrity_report.html\n",
      "   2. c:\\Users\\user\\Downloads\\Notre_Mlops\\testing\\deepchecks_nlp_drift_report.html (checks individuels)\n",
      "   3. c:\\Users\\user\\Downloads\\Notre_Mlops\\testing\\deepchecks_nlp_performance_report.html\n",
      "\n",
      "üèÜ M√©triques Finales:\n",
      "   train_acc: 0.8556\n",
      "   test_acc: 0.7345\n",
      "   test_f1: 0.7079\n",
      "   test_precision: 0.7615\n",
      "   test_recall: 0.6614\n",
      "   overfit_gap: 0.1211\n",
      "\n",
      "üéØ Type de validation: DEEPCHECKS NLP NATIF\n",
      "   ‚úÖ TextData utilis√© (pas DataFrame tabular)\n",
      "   ‚úÖ Analyse s√©mantique du texte brut\n",
      "   ‚úÖ Drift de propri√©t√©s textuelles\n",
      "   ‚úÖ D√©tection outliers NLP\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ VALIDATION DEEPCHECKS NLP TERMIN√âE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìÇ Rapports NLP g√©n√©r√©s:\")\n",
    "print(f\"   1. {TESTING_DIR / 'deepchecks_nlp_integrity_report.html'}\")\n",
    "print(f\"   2. {TESTING_DIR / 'deepchecks_nlp_drift_report.html'} (checks individuels)\")\n",
    "print(f\"   3. {TESTING_DIR / 'deepchecks_nlp_performance_report.html'}\")\n",
    "\n",
    "print(\"\\nüèÜ M√©triques Finales:\")\n",
    "for key, value in metrics.items():\n",
    "    print(f\"   {key}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nüéØ Type de validation: DEEPCHECKS NLP NATIF\")\n",
    "print(\"   ‚úÖ TextData utilis√© (pas DataFrame tabular)\")\n",
    "print(\"   ‚úÖ Analyse s√©mantique du texte brut\")\n",
    "print(\"   ‚úÖ Drift de propri√©t√©s textuelles\")\n",
    "print(\"   ‚úÖ D√©tection outliers NLP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a7bb7c",
   "metadata": {},
   "source": [
    "### üí° Recommandations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9e0ec7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí° Recommandations:\n",
      "   ‚ö†Ô∏è  Overfitting d√©tect√© - envisager:\n",
      "      ‚Ä¢ Augmentation des donn√©es\n",
      "      ‚Ä¢ R√©gularisation plus forte\n",
      "      ‚Ä¢ R√©duction de la complexit√© du mod√®le\n",
      "\n",
      "\n",
      "üîó Ouvrez les rapports HTML pour visualiser les d√©tails\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"üí° Recommandations:\")\n",
    "\n",
    "if metrics['overfit_gap'] > 0.1:\n",
    "    print(\"   ‚ö†Ô∏è  Overfitting d√©tect√© - envisager:\")\n",
    "    print(\"      ‚Ä¢ Augmentation des donn√©es\")\n",
    "    print(\"      ‚Ä¢ R√©gularisation plus forte\")\n",
    "    print(\"      ‚Ä¢ R√©duction de la complexit√© du mod√®le\")\n",
    "    print()\n",
    "\n",
    "if metrics['test_f1'] < 0.7:\n",
    "    print(\"   ‚ö†Ô∏è  F1-Score faible - envisager:\")\n",
    "    print(\"      ‚Ä¢ Features NLP suppl√©mentaires (n-grams, embeddings)\")\n",
    "    print(\"      ‚Ä¢ Fine-tuning TunBERT\")\n",
    "    print(\"      ‚Ä¢ Nettoyage des donn√©es\")\n",
    "    print()\n",
    "\n",
    "if metrics['overfit_gap'] <= 0.1 and metrics['test_f1'] >= 0.7:\n",
    "    print(\"   ‚úÖ Mod√®le performant et bien g√©n√©ralis√©!\")\n",
    "    print(\"      ‚Ä¢ Pr√™t pour la production\")\n",
    "    print(\"      ‚Ä¢ Consid√©rer le d√©ploiement\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nüîó Ouvrez les rapports HTML pour visualiser les d√©tails\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c9ba0d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Documentation\n",
    "\n",
    "Pour plus de d√©tails sur les validations NLP, consultez :\n",
    "- [DEEPCHECKS_VALIDATIONS.md](DEEPCHECKS_VALIDATIONS.md)\n",
    "- [Deepchecks NLP Documentation](https://docs.deepchecks.com/stable/nlp/auto_checks/index.html)\n",
    "\n",
    "### üÜï Diff√©rences avec l'approche Tabular\n",
    "\n",
    "| Aspect | Tabular (ancienne) | NLP (nouvelle) |\n",
    "|--------|-------------------|----------------|\n",
    "| **Import** | `deepchecks.tabular` | `deepchecks.nlp` |\n",
    "| **Donn√©es** | `Dataset(df)` | `TextData(raw_text=...)` |\n",
    "| **Input** | Features num√©riques extraites | **Texte brut** |\n",
    "| **Checks** | `data_integrity()` | `text_data_integrity()` |\n",
    "| **Analyse** | Statistiques colonnes | **Analyse s√©mantique** |\n",
    "| **Drift** | Feature drift | **Property drift + vocabulaire** |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-env (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
