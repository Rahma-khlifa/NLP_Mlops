"""
Script de pr√©traitement des donn√©es - MLOps Election
Charge, nettoie et vectorise les donn√©es textuelles en dialecte tunisien
"""

import os
import sys
import pandas as pd
import numpy as np
import pickle
import re
from pathlib import Path
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from datetime import datetime

# Configuration des chemins
BASE_DIR = Path(__file__).resolve().parent.parent
DATA_DIR = BASE_DIR / 'data'

# Stopwords tunisiens
tunisian_stopwords = [
    'ŸÅŸä', 'ŸÖŸÜ', 'ÿßŸÑŸâ', 'ÿπŸÑŸâ', 'Ÿáÿ∞ÿß', 'Ÿáÿ∞Ÿá', 'ÿ∞ŸÑŸÉ', 'ÿ™ŸÑŸÉ', 'ŸáŸà', 'ŸáŸä',
    'ŸÜÿ≠ŸÜ', 'ŸáŸÖ', 'ÿßŸÜÿ™', 'ÿßŸÜÿ™ŸÖ', 'ÿßŸÜÿ™Ÿä', 'ÿßŸÜÿß', 'ŸáŸÖÿß', 'ŸÉÿßŸÜ', 'ŸÉÿßŸÜÿ™',
    'ŸäŸÉŸàŸÜ', 'ÿ™ŸÉŸàŸÜ', 'ŸÑŸäÿ≥', 'ŸÑŸäÿ≥ÿ™', 'ŸÖÿß', 'ŸÑÿß', 'ŸÑŸÖ', 'ŸÑŸÜ', 'ÿßŸÜ', 'ÿßÿ∞ÿß',
    'ŸÉŸÑ', 'ÿ®ÿπÿ∂', 'ÿπŸÜÿØ', 'ÿ®ÿπÿØ', 'ŸÇÿ®ŸÑ', 'ÿßÿ´ŸÜÿßÿ°', 'ÿÆŸÑÿßŸÑ', 'ŸÖŸÜÿ∞', 'ÿ≠ÿ™Ÿâ',
    'ŸÖÿπ', 'ÿ®ÿØŸàŸÜ', 'ÿ∂ÿØ', 'ÿπŸÜ', 'ÿßŸÑŸä', 'ÿßŸÑŸÑŸä', 'ÿßŸÑŸâ', 'ÿ•ŸÑŸâ', 'ÿπŸÑŸä', 'ÿπŸÑŸâ',
    'Ÿáÿßÿ∞Ÿä', 'Ÿáÿßÿ∞ÿß', 'ŸáŸÉÿß', 'ŸáŸÉÿ©', 'ÿ®ÿ±ÿ¥ÿß', 'Ÿäÿßÿ≥ÿ±', 'ÿ¥ŸàŸäÿ©', 'ÿ≤ÿßÿØÿ©', 'ŸÉŸäŸÖÿß',
    'ÿ®ÿßŸáŸä', 'ŸÖŸàÿ¥', 'ŸÖÿßŸáŸà', 'ŸÖÿßŸáŸä', 'ŸÖÿßŸÜŸäÿ¥', 'ŸÖÿßŸÜÿß', 'ŸÉÿßŸÜÿ¥', 'ŸÖÿßŸÉÿßŸÜÿ¥',
    'ŸàÿßŸÑŸÑŸá', 'Ÿäÿ≤Ÿä', 'ŸÖÿπŸÜÿßŸáÿß', 'ŸäÿπŸÜŸä', 'ÿ®ÿ±ÿß', 'ÿ™Ÿàÿß', 'ÿ™Ÿàÿ©'
]

def clean_arabic_text(text):
    """
    Nettoie le texte arabe/tunisien
    """
    if pd.isna(text):
        return ""
    
    # Convertir en string
    text = str(text)
    
    # Supprimer les URLs
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    
    # Supprimer les mentions (@user)
    text = re.sub(r'@\w+', '', text)
    
    # Supprimer les hashtags (#tag)
    text = re.sub(r'#\w+', '', text)
    
    # Supprimer les emojis et symboles sp√©ciaux
    text = re.sub(r'[^\u0600-\u06FF\s\w]', ' ', text)
    
    # Normaliser les caract√®res arabes
    text = re.sub(r'[ÿ£ÿ•ÿ¢ÿß]', 'ÿß', text)
    text = re.sub(r'[ŸâŸä]', 'Ÿä', text)
    text = re.sub(r'ÿ©', 'Ÿá', text)
    
    # Supprimer les chiffres
    text = re.sub(r'\d+', '', text)
    
    # Supprimer les espaces multiples
    text = re.sub(r'\s+', ' ', text)
    
    # Nettoyer les espaces au d√©but et √† la fin
    text = text.strip()
    
    return text

def load_and_explore_data(filepath):
    """
    Charge et explore les donn√©es
    """
    print("\nüì¶ √âTAPE 1: Chargement des donn√©es")
    print("-" * 80)
    
    df = pd.read_excel(filepath)
    print(f"‚úÖ Donn√©es charg√©es: {df.shape[0]} lignes, {df.shape[1]} colonnes")
    print(f"   Colonnes: {list(df.columns)}")
    
    # V√©rifier les valeurs manquantes
    missing = df.isnull().sum()
    if missing.any():
        print(f"\n‚ö†Ô∏è  Valeurs manquantes:\n{missing[missing > 0]}")
    
    # Distribution des classes
    if 'target' in df.columns:
        print(f"\nüìä Distribution des classes:")
        print(df['target'].value_counts())
        print(f"   Ratio: {df['target'].value_counts(normalize=True).to_dict()}")
    
    return df

def preprocess_text(df, text_col='comments'):
    """
    Nettoie les textes
    """
    print("\nüßπ √âTAPE 2: Nettoyage des textes")
    print("-" * 80)
    
    # Appliquer le nettoyage
    print("   Nettoyage en cours...")
    df['cleaned'] = df[text_col].apply(clean_arabic_text)
    
    # Supprimer les lignes vides apr√®s nettoyage
    before = len(df)
    df = df[df['cleaned'].str.len() > 0].copy()
    after = len(df)
    
    if before != after:
        print(f"   ‚ö†Ô∏è  {before - after} lignes vides supprim√©es")
    
    print(f"‚úÖ Nettoyage termin√©: {len(df)} textes")
    print(f"   Longueur moyenne: {df['cleaned'].str.len().mean():.1f} caract√®res")
    
    return df

def vectorize_text(df, text_col='cleaned', max_features=5000):
    """
    Vectorisation TF-IDF
    """
    print("\nüî¢ √âTAPE 3: Vectorisation TF-IDF")
    print("-" * 80)
    
    # Cr√©er le vectorizer
    vectorizer = TfidfVectorizer(
        max_features=max_features,
        stop_words=tunisian_stopwords,
        ngram_range=(1, 2),
        min_df=2,
        max_df=0.95
    )
    
    # Fit et transform
    X = vectorizer.fit_transform(df[text_col])
    
    print(f"‚úÖ Vectorisation termin√©e")
    print(f"   Shape: {X.shape}")
    print(f"   Features: {len(vectorizer.get_feature_names_out())}")
    print(f"   Sparsit√©: {(1.0 - X.nnz / (X.shape[0] * X.shape[1])) * 100:.2f}%")
    
    return X, vectorizer

def split_data(X, y, test_size=0.15, val_size=0.15, random_state=42):
    """
    Split stratifi√© 70-15-15
    """
    print("\n‚úÇÔ∏è  √âTAPE 4: Split des donn√©es (70-15-15)")
    print("-" * 80)
    
    # Train/temp split (70/30)
    X_train, X_temp, y_train, y_temp = train_test_split(
        X, y, 
        test_size=(test_size + val_size), 
        random_state=random_state,
        stratify=y
    )
    
    # Val/test split (15/15)
    X_val, X_test, y_val, y_test = train_test_split(
        X_temp, y_temp,
        test_size=0.5,
        random_state=random_state,
        stratify=y_temp
    )
    
    print(f"‚úÖ Split termin√©:")
    print(f"   Train: {X_train.shape[0]} ({X_train.shape[0]/X.shape[0]*100:.1f}%)")
    print(f"   Val:   {X_val.shape[0]} ({X_val.shape[0]/X.shape[0]*100:.1f}%)")
    print(f"   Test:  {X_test.shape[0]} ({X_test.shape[0]/X.shape[0]*100:.1f}%)")
    
    # V√©rifier la distribution des classes
    print(f"\n   Distribution Train: {np.bincount(y_train)}")
    print(f"   Distribution Val:   {np.bincount(y_val)}")
    print(f"   Distribution Test:  {np.bincount(y_test)}")
    
    return X_train, X_val, X_test, y_train, y_val, y_test

# Note: This script is now imported by train.py for in-memory preprocessing
# No need to save artifacts to disk - train.py handles that

def main():
    """
    Pipeline principal de pr√©traitement
    NOTE: This script is now primarily imported by train.py for in-memory preprocessing.
    Running it standalone will only validate the preprocessing pipeline without saving artifacts.
    """
    try:
        print("="*80)
        print("üß™ PREPROCESSING VALIDATION - Standalone Mode")
        print("="*80)
        print("‚ö†Ô∏è  Note: This validates preprocessing without saving artifacts.")
        print("   For training, use: python scripts/train.py")
        print()
        
        # Charger les donn√©es
        data_path = DATA_DIR / 'version1.xlsx'
        if not data_path.exists():
            raise FileNotFoundError(f"Fichier de donn√©es non trouv√©: {data_path}")
        
        df = load_and_explore_data(data_path)
        
        # Pr√©traiter les textes
        df = preprocess_text(df, text_col='comments')
        
        # Vectorisation TF-IDF
        X, vectorizer = vectorize_text(df, text_col='cleaned', max_features=5000)
        
        # Extraire les labels
        y = df['target'].values
        
        # Split des donn√©es
        X_train, X_val, X_test, y_train, y_val, y_test = split_data(
            X, y, test_size=0.15, val_size=0.15, random_state=42
        )
        
        print("\n" + "="*80)
        print("‚úÖ PREPROCESSING VALIDATION SUCCESSFUL!")
        print("="*80)
        print(f"üìä Data shapes validated:")
        print(f"   Train: {X_train.shape}")
        print(f"   Val:   {X_val.shape}")
        print(f"   Test:  {X_test.shape}")
        print(f"\nüí° To train models, run: python scripts/train.py")
        
        return 0
        
    except Exception as e:
        print(f"\n‚ùå ERREUR: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    sys.exit(main())
